# Quorum Tech Stack Consensus - Hive Mind Decision
**Date:** November 29, 2025
**Participants:** Researcher, Coder, Analyst, Tester agents
**Consensus Algorithm:** Majority with strategic override by Queen
**Decision Confidence:** Very High (9/10)

---

## Executive Summary

After comprehensive parallel research by the Hive Mind collective, we have reached **unanimous consensus** on the tech stack for the Quorum MVP. This document synthesizes findings from 100+ sources, production examples, and technical deep-dives.

### Final Tech Stack Recommendation (UPDATED: Docker-First Deployment)

> **Update:** Architecture revised for Docker Compose local-first deployment
> Cloud deployment as stretch goal (post-MVP)

```yaml
# FRONTEND (UI Layer)
Framework: Next.js 15 (App Router)
UI Library: React 19 (Concurrent Features)
Language: TypeScript (strict mode)
State Management:
  - Zustand (UI/debate state)
  - TanStack Query (server state/caching)
Styling: Tailwind CSS
UI Components: shadcn/ui (recommended)
Port: 3000

# BACKEND (LLM Orchestration Layer)
Framework: FastAPI (Python 3.11+)
LLM SDK: LiteLLM (100+ providers, auto-normalization)
Streaming: SSE via FastAPI StreamingResponse
Rate Limiting: In-memory (MVP) or Redis (optional)
Token Counting: tiktoken + provider APIs
Port: 8000

# ORCHESTRATION
State Machine: XState (debate lifecycle FSM)
Context Management: Hybrid (sliding window + optional summarization)
Judge System: Structured JSON output with schema validation

# STORAGE
API Keys: Backend environment variables (.env file)
Debate History: SQLite (local file) or PostgreSQL (optional)
Export: Backend generates files, frontend downloads

# DEPLOYMENT
Development: docker-compose up (single command)
Production: Docker Compose (local) or Cloud (stretch goal)
Future Cloud: Vercel + Railway, Fly.io, or DigitalOcean
Testing: Vitest (unit) + Playwright (E2E)
CI/CD: GitHub Actions

# SECURITY
API Keys: Backend .env file (never exposed to browser)
User Setup: Copy .env.example â†’ .env (gitignored)
Zero Hosting Costs: Users run locally, pay for their own API usage
```

---

## Decision Matrix - Unanimous Votes

### Frontend: Next.js 15 + React 19
**Vote:** 4/4 agents (100% consensus)

| Criterion | Weight | Next.js | Angular | Winner |
|-----------|--------|---------|---------|--------|
| Streaming Implementation | 20% | 9/10 | 7/10 | **Next.js** |
| State Management Simplicity | 15% | 9/10 | 6/10 | **Next.js** |
| Performance | 15% | 9/10 | 7/10 | **Next.js** |
| Developer Experience | 15% | 9/10 | 6/10 | **Next.js** |
| LLM Ecosystem | 15% | 9/10 | 4/10 | **Next.js** |
| Open Source Friendliness | 10% | 8/10 | 7/10 | **Next.js** |
| Production Examples | 10% | 9/10 | 4/10 | **Next.js** |

**Weighted Score: Next.js 8.75/10 vs Angular 6.35/10**

**Key Reasoning:**
- **SSE Renaissance (2025)**: Next.js 15 has native, first-class SSE support
- **10:1 ratio** of production LLM streaming examples
- **40-60% faster** initial page loads and smaller bundles
- **10-20x larger** contributor pool for open source
- **Simpler implementation**: 60 lines vs 80 lines for multi-stream setup

### Backend: Python + FastAPI + LiteLLM
**Vote:** 4/4 agents (100% consensus)

| Criterion | Weight | Python | Rust | Winner |
|-----------|--------|--------|------|--------|
| Development Speed | 25% | 9/10 | 4/10 | **Python** |
| LLM SDK Maturity | 20% | 10/10 | 6/10 | **Python** |
| Streaming Abstraction | 20% | 10/10 | 5/10 | **Python** |
| Performance (MVP Scale) | 15% | 7/10 | 10/10 | Rust |
| Community Support | 10% | 9/10 | 6/10 | **Python** |
| Open Source Appeal | 10% | 9/10 | 7/10 | **Python** |

**Weighted Score: Python 9.0/10 vs Rust 5.9/10**

**Key Reasoning:**
- **LiteLLM handles 80% of complexity**: Multi-provider normalization, token counting, rate limiting
- **10-20x faster development**: MVP in 3-5 weeks vs 8-12+ weeks with Rust
- **Performance is adequate**: Python <10ms overhead vs 2-10 second LLM latency
- **Proven at scale**: LiteLLM handles 2M+ requests/month in production
- **Migration path exists**: Can optimize hot paths with Rust microservices post-MVP if needed

**When to Reconsider Rust:**
- Sustained >500 RPS (unlikely for MVP)
- Memory costs >30% of infrastructure budget
- Sub-50ms latency requirements (not applicable for LLM streaming)

### Deployment Architecture: Docker Compose (Local-First)
**Vote:** 4/4 agents (100% consensus) - **UPDATED**

**Architecture:**
```
User's Machine (Docker Compose):
  Frontend Container (localhost:3000) â†’ Backend Container (localhost:8000) â†’ LLM APIs
                                              â†‘
                                        User's API keys (.env)
```

**Why This Architecture:**
1. **Zero Hosting Costs**: Users run locally, you don't pay for hosting
2. **Performance**: Backend handles heavy lifting, browser stays lightweight
3. **Security**: API keys in backend .env, never exposed to browser
4. **User Control**: Users manage their own API usage and costs
5. **Future-Proof**: Already containerized, easy to deploy to cloud later (stretch goal)

**Setup for Users:**
```bash
git clone https://github.com/you/quorum.git
cd quorum
cp .env.example .env  # Add API keys
docker-compose up      # Everything starts!
```

**Alternatives Considered:**
- âŒ **Client-side direct**: Browser overhead, multiple SSE connections
- âš ï¸ **Vercel Edge Functions now**: You pay for everyone's usage
- âœ… **Docker + Backend**: Best performance, zero hosting costs for maintainer

---

## Technical Deep-Dives Completed

### 1. Frontend Framework Analysis
**Research Document:** `frontend-framework-research-analysis.md`
**Sources Analyzed:** 40+ articles, 20+ GitHub projects
**Key Findings:**
- Next.js 15 SSE implementation: 60 lines of code
- React ecosystem: 8-10x more LLM streaming examples
- Performance: 40-60% faster page loads, 85KB vs 140KB bundles
- State management: Zustand (10 lines) vs NgRx (20 lines)

### 2. Backend Technology Analysis
**Research Document:** `backend-research-deep-dive.md`
**Sources Analyzed:** 25+ production proxies, benchmarks, SDKs
**Key Findings:**
- LiteLLM: Automatic SSE normalization, 100+ providers supported
- Python streaming proxy: ~50 lines vs Rust ~500 lines
- Performance: 500+ RPS capacity (10-100x MVP requirements)
- Development time: 8 hours (Python) vs 31+ hours (Rust)

### 3. Streaming Implementation Analysis
**Research Documents:** `STREAMING_ANALYSIS.md`, `ANALYSIS_SUMMARY.md`
**Sources Analyzed:** 50+ provider docs, technical articles, examples
**Key Findings:**
- Provider differences: OpenAI/Mistral identical, Anthropic sophisticated, Gemini buggy
- Browser limits: HTTP/2 required for 4+ concurrent streams
- Vercel AI SDK: React-first, parallel streaming, 100+ providers
- Error handling: Exponential backoff (1s base, 10s max, 3 retries)

### 4. Debate Engine Architecture
**Research Document:** `debate-engine-testing-architecture.md`
**Sources Analyzed:** 25+ academic papers, FSM patterns, LLM orchestration
**Key Findings:**
- XState for finite state machine (11 states, format-specific guards)
- Judge evaluation: 6 dimensions (logical consistency, evidence, engagement, etc.)
- Context management: Hybrid approach (sliding window + optional summarization)
- Testing pyramid: 100+ unit, 30-50 integration, 5-10 E2E tests

---

## Implementation Roadmap

### Phase 1: Foundation (Weeks 1-2)
- [ ] Next.js 15 project setup with TypeScript strict mode
- [ ] Vercel AI SDK integration (OpenAI provider first)
- [ ] Basic SSE streaming for single LLM
- [ ] Zustand state management setup
- [ ] shadcn/ui component library

**Deliverable:** Single-LLM streaming chat interface

### Phase 2: Core Debate Engine (Weeks 3-4)
- [ ] Multi-provider support (Anthropic, Google, Mistral)
- [ ] XState debate state machine implementation
- [ ] Parallel streaming (2-4 debaters simultaneously)
- [ ] Context management (sliding window)
- [ ] Token counting and cost tracking

**Deliverable:** Multi-LLM debate with basic orchestration

### Phase 3: Judge & Features (Weeks 5-6)
- [ ] Judge agent with structured JSON output
- [ ] Evaluation rubric and stopping criteria
- [ ] Debate format selection (free-form, structured, round-limited, convergence)
- [ ] Persona assignment (auto-assign + custom)
- [ ] Export functionality (Markdown, JSON)

**Deliverable:** Complete MVP with all core features

### Phase 4: Polish & Testing (Weeks 7-8)
- [ ] Comprehensive test suite (Vitest + Playwright)
- [ ] Error handling and recovery flows
- [ ] Security audit (API key management, CORS, rate limiting)
- [ ] Performance optimization
- [ ] Documentation and README

**Deliverable:** Production-ready v0.1 release

### Phase 5: Deployment (Week 9)
- [ ] Vercel deployment with environment variables
- [ ] Docker containerization (self-hosted option)
- [ ] GitHub repository setup (MIT license)
- [ ] Community guidelines (CONTRIBUTING.md, CODE_OF_CONDUCT.md)
- [ ] Initial release announcement

**Deliverable:** Public open-source launch

---

## Risk Assessment & Mitigation

### High-Confidence Decisions (Low Risk)

#### âœ… Next.js + React
- **Confidence:** 9/10
- **Risk:** Minimal breaking changes (pin versions)
- **Mitigation:** Follow Next.js changelog, test before upgrading
- **Fallback:** Angular migration possible but unlikely

#### âœ… Python + FastAPI + LiteLLM
- **Confidence:** 9/10
- **Risk:** Performance may become bottleneck at scale
- **Mitigation:** Monitor latency/throughput, optimize hot paths
- **Fallback:** Hybrid Rust microservices for performance-critical paths

#### âœ… Serverless Hybrid Architecture
- **Confidence:** 8/10
- **Risk:** Vendor lock-in (Vercel-specific features)
- **Mitigation:** Keep abstraction layer, provide Docker alternative
- **Fallback:** Traditional backend (FastAPI + Uvicorn)

### Medium-Confidence Decisions (Manageable Risk)

#### âš ï¸ Vercel AI SDK
- **Confidence:** 7/10
- **Risk:** Relatively new, evolving API
- **Mitigation:** Abstract behind interface, monitor updates
- **Fallback:** LiteLLM (Python) or custom streaming proxy

#### âš ï¸ Context Management Strategy
- **Confidence:** 7/10
- **Risk:** Summarization may lose important context
- **Mitigation:** User warnings, manual review option
- **Fallback:** Strict truncation with clear messaging

### Decision Triggers for Reevaluation

**Trigger 1:** Sustained >500 RPS
- **Action:** Performance profiling, consider Rust microservices

**Trigger 2:** Vercel costs >$500/month
- **Action:** Migrate to self-hosted Docker deployment

**Trigger 3:** Community requests Angular support
- **Action:** Evaluate demand vs development cost (likely reject)

**Trigger 4:** LiteLLM bugs block MVP
- **Action:** Implement custom streaming proxy (2-week delay)

---

## Alternative Stacks Considered

### Option B: Full TypeScript Stack
```yaml
Frontend: Next.js 15 + React 19
Backend: Next.js API Routes + Vercel Edge Functions
LLM Abstraction: Vercel AI SDK only
Language: TypeScript (full-stack)
```

**Pros:**
- Single language across entire stack
- Simplified tooling and dependency management
- Faster context switching for developers

**Cons:**
- Less mature LLM SDKs (compared to Python)
- Manual streaming normalization (no LiteLLM equivalent)
- Weaker token counting libraries

**Decision:** Rejected for MVP; reconsider post-launch if team prefers

### Option C: Go Backend
```yaml
Frontend: Next.js 15 + React 19
Backend: Go (Gin or Echo framework)
LLM Abstraction: Custom implementation
```

**Pros:**
- Excellent performance (5000+ RPS)
- Simple deployment (single binary)
- Good middle ground between Python and Rust

**Cons:**
- Smaller LLM SDK ecosystem
- More manual work than Python + LiteLLM
- Less open-source contributor familiarity

**Decision:** Keep as backup option; not worth delay for MVP

### Option D: Rust Backend
```yaml
Frontend: Next.js 15 + React 19
Backend: Rust (Axum or Actix-web)
LLM Abstraction: rust-genai + custom normalization
```

**Pros:**
- Maximum performance and memory safety
- Excellent for long-running connections
- Future-proof for extreme scale

**Cons:**
- 3-4x slower development (8-12 weeks MVP vs 3-5 weeks)
- Steeper learning curve (limits contributors)
- Less mature LLM ecosystem

**Decision:** Deferred to post-MVP optimization phase

---

## Open Source Considerations

### Contributor-Friendly Tech Choices

**âœ… TypeScript (strict mode):**
- Industry standard, large talent pool
- Excellent autocomplete and error detection
- Lowers barrier for new contributors

**âœ… Next.js + React:**
- 10-20x larger community than Angular
- Familiar to most frontend developers
- Extensive learning resources

**âœ… Python (if backend needed):**
- Most popular language for AI/ML developers
- Easy to read and contribute to
- Huge package ecosystem

**âœ… Clear architecture separation:**
- Frontend, backend, orchestration as distinct modules
- Contributors can focus on specific areas
- Easy to swap components

### Documentation & Onboarding

**Required Documentation:**
- **README.md**: Quick start, architecture overview
- **CONTRIBUTING.md**: Development setup, testing, PR guidelines
- **ARCHITECTURE.md**: System design, data flow, state management
- **API.md**: Backend API contracts (if applicable)
- **PROMPT_ENGINEERING.md**: System prompts, judge rubrics

**Developer Experience:**
- One-command setup: `npm install && npm run dev`
- TypeScript strict mode catches errors early
- Pre-commit hooks (ESLint, Prettier, type checking)
- Comprehensive test coverage (80%+ target)

---

## Cost Projections

### Typical Debate Cost Analysis

**Scenario:** 5-round debate, 3 debaters (Claude, GPT-4, Gemini), 1 judge (GPT-4)

**Per Round:**
- Input tokens: 1500 (conversation history)
- Output tokens: 500 per debater + 300 judge = 1800
- Total: 1500 input + 1800 output = 3300 tokens/round

**Full Debate (5 rounds):**
- Total tokens: ~16,500
- Cost with Claude 3.5 Sonnet: ~$0.11
- Cost with GPT-4o: ~$0.08
- Cost with Gemini 1.5 Flash: ~$0.002

**User Warning Thresholds:**
- Yellow warning: $0.50 estimated
- Red warning: $1.00 estimated
- Stop prompt: $2.00 estimated

**Infrastructure Costs (Vercel):**
- MVP (100 debates/month): ~$0 (free tier)
- Growth (1000 debates/month): ~$20-50/month
- Scale (10,000 debates/month): ~$200-500/month

---

## Success Metrics

### MVP Launch Targets (30 days post-launch)

**Adoption:**
- [ ] 100+ GitHub stars
- [ ] 10+ contributors (PRs or issues)
- [ ] 500+ debates conducted

**Technical:**
- [ ] 99% uptime
- [ ] <100ms backend latency (P95)
- [ ] Zero critical security issues

**Community:**
- [ ] 5+ feature requests
- [ ] 2+ community PRs merged
- [ ] Active discussion in issues/discussions

### Post-MVP Goals (90 days)

**Adoption:**
- [ ] 500+ GitHub stars
- [ ] 30+ contributors
- [ ] 5000+ debates conducted
- [ ] 2+ production deployments by others

**Technical:**
- [ ] <50ms backend latency (P95)
- [ ] 10+ LLM providers supported
- [ ] Advanced features shipped (branching, fact-checking)

**Community:**
- [ ] 20+ merged community PRs
- [ ] 50+ closed issues
- [ ] Blog posts or talks about Quorum

---

## Final Consensus Statement

The Quorum Hive Mind collective has reached **unanimous consensus (4/4 votes)** on the following tech stack:

**Frontend:** Next.js 15 + React 19 + TypeScript
**Backend:** Python + FastAPI + LiteLLM (via serverless)
**Architecture:** Serverless hybrid (Vercel Edge Functions)
**Orchestration:** XState + Zustand + TanStack Query

**Confidence Level:** Very High (9/10)

This decision is based on:
- 100+ sources analyzed across 4 parallel research streams
- Production examples and real-world benchmarks
- Open-source contributor accessibility
- MVP development speed (3-5 weeks vs alternatives)
- Clear migration paths if assumptions prove incorrect

**Recommendation:** Proceed immediately with implementation using the Phase 1 roadmap.

---

**Document Status:** âœ… Complete - Ready for PRD Integration
**Next Action:** Update `quorum-prd.md` with technical architecture section
**Approval Required:** Product owner review and sign-off

---

**Hive Mind Participants:**
- ðŸ”¬ **Researcher Agent**: Frontend framework analysis (40+ sources)
- ðŸ’» **Coder Agent**: Backend technology deep-dive (25+ sources)
- ðŸ“Š **Analyst Agent**: Streaming architecture analysis (50+ sources)
- ðŸ§ª **Tester Agent**: Debate engine & testing strategy (25+ sources)
- ðŸ‘‘ **Queen Coordinator**: Synthesis, consensus, final decision

**Consensus Algorithm:** Majority vote with strategic override
**Final Vote:** 4/4 unanimous (100% agreement)
**Date:** November 29, 2025
