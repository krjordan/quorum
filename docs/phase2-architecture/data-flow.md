# Phase 2: Data Flow Architecture

## Overview

Comprehensive data flow for multi-LLM debate system, showing how user actions propagate through frontend state machines, backend orchestration, LLM APIs, and back to real-time UI updates.

## High-Level Data Flow Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           USER INTERFACE                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚ Config Panel â”‚  â”‚ Debate Arena â”‚  â”‚ Cost Tracker â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚         â”‚                  â”‚                  â”‚                       â”‚
â”‚         â”‚ User Config      â”‚ Display Updates  â”‚ Cost Updates         â”‚
â”‚         â–¼                  â–¼                  â–¼                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚              XState Debate Machine                    â”‚           â”‚
â”‚  â”‚  States: idle â†’ initializing â†’ debating â†’ completed  â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚         â”‚ Send Events            â”‚ State Updates                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                        â”‚
          â”‚ START_DEBATE           â”‚ SSE Events
          â”‚ (POST /debates)        â”‚ (GET /debates/:id/stream)
          â–¼                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        BACKEND API LAYER                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚           Debate Orchestrator (FastAPI)                â”‚         â”‚
â”‚  â”‚  - Validate config                                     â”‚         â”‚
â”‚  â”‚  - Initialize participants                             â”‚         â”‚
â”‚  â”‚  - Manage SSE connections                              â”‚         â”‚
â”‚  â”‚  - Coordinate rounds                                   â”‚         â”‚
â”‚  â””â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚      â”‚                  â”‚                  â”‚                         â”‚
â”‚      â”‚ Store State      â”‚ Parallel LLM     â”‚ Token Count            â”‚
â”‚      â–¼                  â”‚ Calls            â–¼                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â–¼          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚ PostgreSQL â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚ Token Counter  â”‚               â”‚
â”‚  â”‚ - Debates  â”‚    â”‚ Redis  â”‚      â”‚ (tiktoken)     â”‚               â”‚
â”‚  â”‚ - Rounds   â”‚    â”‚ Pub/Subâ”‚      â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚  â”‚ - Verdicts â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚                         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â”‚ Cost Calculation        â”‚
â”‚                                             â–¼                         â”‚
â”‚                                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚                                      â”‚ Cost Tracker   â”‚               â”‚
â”‚                                      â”‚ Service        â”‚               â”‚
â”‚                                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â”‚ LLM API Calls (parallel)
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      EXTERNAL LLM APIs                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚  Anthropic   â”‚  â”‚    OpenAI    â”‚  â”‚    Google    â”‚              â”‚
â”‚  â”‚   Claude     â”‚  â”‚    GPT-4     â”‚  â”‚    Gemini    â”‚              â”‚
â”‚  â”‚              â”‚  â”‚              â”‚  â”‚              â”‚              â”‚
â”‚  â”‚  Streaming   â”‚  â”‚  Streaming   â”‚  â”‚  Streaming   â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚         â”‚ SSE Chunks       â”‚                  â”‚                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                  â”‚                  â”‚
          â”‚ Stream text      â”‚                  â”‚
          â–¼                  â–¼                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    BACKEND STREAM PROCESSOR                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚         SSE Event Multiplexer                          â”‚         â”‚
â”‚  â”‚  - Buffer LLM chunks                                   â”‚         â”‚
â”‚  â”‚  - Publish to Redis                                    â”‚         â”‚
â”‚  â”‚  - Forward to all connected clients                    â”‚         â”‚
â”‚  â”‚  - Track tokens in real-time                           â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚ SSE Events
              â”‚ (participant, cost_update, round_complete, etc.)
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   FRONTEND SSE HANDLER                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚         useDebateSSE Hook                              â”‚         â”‚
â”‚  â”‚  - Parse SSE events                                    â”‚         â”‚
â”‚  â”‚  - Route to XState machine                             â”‚         â”‚
â”‚  â”‚  - Buffer chunks in Zustand                            â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚ Dispatch to XState + Zustand
              â–¼
        [Back to USER INTERFACE - Loop Closed]
```

---

## Detailed Flow Breakdowns

### Flow 1: Debate Initialization

```
[User] Fill Config Form
   â”‚
   â”œâ”€ Topic: "Should AI be regulated?"
   â”œâ”€ Participants: Claude (for), GPT-4 (against)
   â”œâ”€ Judge: Claude Opus
   â”œâ”€ Max Rounds: 5
   â””â”€ Cost Limit: $5.00
   â”‚
   â–¼
[Frontend] Validate Input
   â”œâ”€ Check topic length (10-500 chars)
   â”œâ”€ Verify 2-4 participants
   â””â”€ Validate cost limits
   â”‚
   â–¼
[XState] send({ type: 'START_DEBATE', config })
   â”‚
   â–¼ State: idle â†’ initializing
   â”‚
[Frontend] POST /api/v1/debates
   â”‚
   â”‚ Request Body:
   â”‚ {
   â”‚   topic: "Should AI be regulated?",
   â”‚   format: "oxford",
   â”‚   participants: [
   â”‚     { name: "Pro (Claude)", model: { provider: "anthropic", ... } },
   â”‚     { name: "Con (GPT-4)", model: { provider: "openai", ... } }
   â”‚   ],
   â”‚   judge: { name: "Judge", model: { provider: "anthropic", ... } },
   â”‚   config: { maxRounds: 5, costLimit: 5.0, ... }
   â”‚ }
   â”‚
   â–¼
[Backend] Debate Orchestrator
   â”œâ”€ Generate debate_id: "deb_abc123"
   â”œâ”€ Validate API keys exist
   â”œâ”€ Initialize LLM clients
   â”‚  â”œâ”€ AnthropicClient(model="claude-3-5-sonnet")
   â”‚  â””â”€ OpenAIClient(model="gpt-4-turbo")
   â”œâ”€ Store debate in PostgreSQL:
   â”‚  â””â”€ INSERT INTO debates (id, topic, config, status)
   â””â”€ Initialize cost tracker:
      â””â”€ CostTracker(limit=5.0, warn_at=3.0)
   â”‚
   â–¼
[Backend] Return 201 Created
   â”‚ Response:
   â”‚ {
   â”‚   id: "deb_abc123",
   â”‚   status: "initializing",
   â”‚   streamUrl: "/api/v1/debates/deb_abc123/stream",
   â”‚   participants: [...],
   â”‚   ...
   â”‚ }
   â”‚
   â–¼
[Frontend] Store debateId in XState context
   â”‚
   â–¼
[Frontend] Open SSE Connection
   â”‚ GET /api/v1/debates/deb_abc123/stream
   â”‚
   â–¼
[XState] send({ type: 'INIT_COMPLETE' })
   â”‚
   â–¼ State: initializing â†’ awaitingArguments
```

---

### Flow 2: Round Execution (Parallel LLM Calls)

```
[Backend] Round Start (Round 1)
   â”‚
   â”œâ”€ Prepare prompts for each participant
   â”‚  â”œâ”€ Participant 1 (Claude): "You are debating FOR AI regulation..."
   â”‚  â””â”€ Participant 2 (GPT-4): "You are debating AGAINST regulation..."
   â”‚
   â–¼
[Backend] Parallel LLM API Calls (asyncio.gather)
   â”‚
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚                      â”‚                      â”‚
   â–¼                      â–¼                      â–¼
[Anthropic API]      [OpenAI API]         [Google API]
POST /v1/messages    POST /v1/chat/       POST /v1/generate
   â”‚                 completions              â”‚
   â”‚ stream=true        â”‚ stream=true          â”‚
   â”‚                    â”‚                      â”‚
   â–¼                    â–¼                      â–¼
SSE Stream 1       SSE Stream 2         SSE Stream 3
   â”‚                    â”‚                      â”‚
   â”‚ Chunk: "Government" â”‚ Chunk: "Innovation"  â”‚
   â”‚ Chunk: " regulation"â”‚ Chunk: " requires"   â”‚
   â”‚ ...                 â”‚ ...                  â”‚
   â”‚                    â”‚                      â”‚
   â–¼                    â–¼                      â–¼
[Backend] Stream Aggregator
   â”‚
   â”œâ”€ For each chunk received:
   â”‚  â”œâ”€ Count tokens (tiktoken)
   â”‚  â”œâ”€ Calculate cost increment
   â”‚  â”œâ”€ Publish to Redis (for horizontal scaling)
   â”‚  â””â”€ Forward to SSE clients
   â”‚
   â–¼
[Backend] Emit SSE Events (multiplexed)
   â”‚
   â”œâ”€ event: participant
   â”‚  data: { participantId: "part_1", chunk: "Government", done: false }
   â”‚
   â”œâ”€ event: participant
   â”‚  data: { participantId: "part_2", chunk: "Innovation", done: false }
   â”‚
   â”œâ”€ event: cost_update
   â”‚  data: { totalCost: 0.05, costByModel: {...} }
   â”‚
   â””â”€ ... (continue streaming)
   â”‚
   â–¼
[Frontend] SSE Handler (useDebateSSE)
   â”‚
   â”œâ”€ Parse events
   â”œâ”€ Route to appropriate handler
   â”‚
   â–¼
[Zustand] Buffer chunks
   â”‚ streamBuffers.set("part_1", currentText + newChunk)
   â”‚
   â–¼
[React] Re-render ParticipantPanel
   â”‚ Display buffered text with typewriter effect
   â”‚
   â–¼
[XState] When all streams complete:
   â”‚ send({ type: 'ROUND_COMPLETE', round: {...} })
   â”‚
   â–¼ State: debating (round 1) â†’ debating (round 2)
```

---

### Flow 3: Cost Tracking & Warning

```
[Backend] Token Counter (real-time during streaming)
   â”‚
   â”œâ”€ For each LLM response chunk:
   â”‚  â”œâ”€ Count tokens: tiktoken.encode(chunk)
   â”‚  â”œâ”€ Accumulate: total_tokens += chunk_tokens
   â”‚  â””â”€ Calculate cost:
   â”‚     â”œâ”€ Claude: input=$3/1M, output=$15/1M
   â”‚     â”œâ”€ GPT-4: input=$10/1M, output=$30/1M
   â”‚     â””â”€ total_cost += (tokens * price_per_token)
   â”‚
   â–¼
[Backend] Cost Tracker Service
   â”‚
   â”œâ”€ Update totals:
   â”‚  â”œâ”€ total_cost: $2.85
   â”‚  â”œâ”€ cost_by_model:
   â”‚  â”‚  â”œâ”€ "anthropic/claude": $1.20
   â”‚  â”‚  â””â”€ "openai/gpt-4": $1.65
   â”‚  â””â”€ token_usage: { total: 15000, by_model: {...} }
   â”‚
   â”œâ”€ Check thresholds:
   â”‚  â”œâ”€ warn_at: $3.00 (not reached yet)
   â”‚  â””â”€ limit: $5.00 (not reached)
   â”‚
   â–¼
[Backend] Emit SSE cost_update every 5 seconds
   â”‚
   â”‚ event: cost_update
   â”‚ data: {
   â”‚   totalCost: 2.85,
   â”‚   costByModel: { "anthropic/claude": 1.20, "openai/gpt-4": 1.65 },
   â”‚   tokensUsed: { total: 15000, ... }
   â”‚ }
   â”‚
   â–¼
[Frontend] CostTracker Component
   â”‚
   â”œâ”€ Display: "$2.85 / $5.00"
   â”œâ”€ Progress bar: 57% (green)
   â””â”€ Model breakdown:
      â”œâ”€ Claude: $1.20 (8,000 tokens)
      â””â”€ GPT-4: $1.65 (7,000 tokens)

   ... Later in debate ...

[Backend] Cost exceeds warning threshold ($3.00)
   â”‚
   â”œâ”€ Create warning:
   â”‚  â””â”€ CostWarning(threshold=3.0, current=3.15)
   â”‚
   â–¼
[Backend] Emit SSE cost_warning
   â”‚
   â”‚ event: cost_warning
   â”‚ data: {
   â”‚   threshold: 3.0,
   â”‚   currentCost: 3.15,
   â”‚   percentOfLimit: 63,
   â”‚   message: "Cost exceeded 60% of limit"
   â”‚ }
   â”‚
   â–¼
[Frontend] Display Modal Warning
   â”‚
   â”œâ”€ Title: "Cost Warning"
   â”œâ”€ Message: "You've used $3.15 of $5.00 (63%)"
   â”œâ”€ Actions:
   â”‚  â”œâ”€ [Continue] â†’ send({ type: 'ACKNOWLEDGE_WARNING' })
   â”‚  â””â”€ [Stop Debate] â†’ send({ type: 'STOP' })
   â”‚
   â–¼
[User] Clicks "Continue"
   â”‚
   â–¼
[XState] Mark warning as acknowledged
   â”‚ context.costTracker.warnings[0].acknowledged = true
   â”‚
   â–¼
[Debate Continues...]

   ... If cost limit exceeded ($5.00) ...

[Backend] Cost limit check (guard in orchestrator)
   â”‚
   â”œâ”€ total_cost: $5.10 >= limit: $5.00
   â”‚
   â–¼
[Backend] Force stop debate
   â”‚
   â”œâ”€ Cancel active LLM streams
   â”œâ”€ Trigger judge evaluation early
   â”‚
   â–¼
[Backend] Emit SSE error event
   â”‚
   â”‚ event: error
   â”‚ data: {
   â”‚   type: "cost_limit",
   â”‚   message: "Cost limit of $5.00 exceeded",
   â”‚   retryable: false
   â”‚ }
   â”‚
   â–¼
[XState] Transition to error state
   â”‚ State: debating â†’ error
   â”‚
   â–¼
[Frontend] Display error message + final costs
```

---

### Flow 4: Judge Evaluation

```
[Backend] All rounds completed (or manual stop)
   â”‚
   â”œâ”€ Compile full debate transcript
   â”‚  â”œâ”€ Round 1:
   â”‚  â”‚  â”œâ”€ Claude: "Government regulation..."
   â”‚  â”‚  â””â”€ GPT-4: "Innovation requires freedom..."
   â”‚  â”œâ”€ Round 2:
   â”‚  â”‚  â””â”€ ...
   â”‚  â””â”€ Round 5 (final)
   â”‚
   â–¼
[Backend] Prepare judge prompt
   â”‚
   â”‚ system: "You are an impartial debate judge. Evaluate based on:
   â”‚          - Argument strength
   â”‚          - Evidence quality
   â”‚          - Logical consistency
   â”‚          - Rebuttal effectiveness"
   â”‚
   â”‚ user: "Here is the full debate transcript:\n\n[transcript]\n\n
   â”‚         Provide your verdict with scores for each participant."
   â”‚
   â–¼
[Backend] Call Judge LLM API (streaming)
   â”‚
   â”‚ POST https://api.anthropic.com/v1/messages
   â”‚ {
   â”‚   model: "claude-3-opus-20240229",
   â”‚   messages: [...],
   â”‚   stream: true
   â”‚ }
   â”‚
   â–¼
[Anthropic API] Stream judge response
   â”‚
   â”‚ Chunk: "After careful consideration"
   â”‚ Chunk: " of both arguments,"
   â”‚ Chunk: " the winner is Pro-Regulation"
   â”‚ ...
   â”‚
   â–¼
[Backend] Forward judge chunks via SSE
   â”‚
   â”‚ event: judge
   â”‚ data: { chunk: "After careful consideration", done: false }
   â”‚
   â”‚ event: judge
   â”‚ data: { chunk: " of both arguments,", done: false }
   â”‚
   â”‚ ... (continue streaming)
   â”‚
   â–¼
[Frontend] JudgePanel displays streaming verdict
   â”‚
   â”‚ "After careful consideration of both arguments, the winner is..."
   â”‚ (typewriter effect)
   â”‚
   â–¼
[Backend] Judge stream completes
   â”‚
   â”œâ”€ Parse verdict from response
   â”‚  â”œâ”€ winner: "part_1" (Claude)
   â”‚  â”œâ”€ scores:
   â”‚  â”‚  â”œâ”€ Claude: 85/100
   â”‚  â”‚  â””â”€ GPT-4: 78/100
   â”‚  â”œâ”€ reasoning: "Pro-Regulation presented stronger evidence..."
   â”‚  â””â”€ criteria:
   â”‚     â”œâ”€ "Argument Strength": "Both strong, Pro had edge"
   â”‚     â”œâ”€ "Evidence Quality": "Pro cited more sources"
   â”‚     â””â”€ ...
   â”‚
   â”œâ”€ Count judge tokens: 1,200 tokens
   â”œâ”€ Calculate judge cost: $0.04
   â”œâ”€ Update total cost: $5.14
   â”‚
   â–¼
[Backend] Store verdict in PostgreSQL
   â”‚
   â”‚ INSERT INTO verdicts (debate_id, winner, scores, reasoning, ...)
   â”‚
   â–¼
[Backend] Emit SSE verdict event
   â”‚
   â”‚ event: verdict
   â”‚ data: {
   â”‚   winner: "part_1",
   â”‚   scores: {
   â”‚     "part_1": { score: 85, strengths: [...], weaknesses: [...] },
   â”‚     "part_2": { score: 78, ... }
   â”‚   },
   â”‚   reasoning: "Pro-Regulation presented...",
   â”‚   criteria: {...},
   â”‚   tokensUsed: 1200
   â”‚ }
   â”‚
   â–¼
[XState] send({ type: 'VERDICT_READY', verdict })
   â”‚
   â–¼ State: judgeEvaluating â†’ completed
   â”‚
   â–¼
[Backend] Emit SSE complete event
   â”‚
   â”‚ event: complete
   â”‚ data: {
   â”‚   debateId: "deb_abc123",
   â”‚   totalRounds: 5,
   â”‚   finalCost: 5.14,
   â”‚   duration: 180,
   â”‚   verdict: {...}
   â”‚ }
   â”‚
   â–¼
[Frontend] Display DebateResults
   â”‚
   â”œâ”€ Winner announcement: "ðŸ† Pro-Regulation (Claude)"
   â”œâ”€ Score cards with breakdown
   â”œâ”€ Full transcript viewer
   â”œâ”€ Final cost summary
   â””â”€ Export options (JSON, Markdown, HTML)
   â”‚
   â–¼
[Frontend] Close SSE connection
   â”‚ eventSource.close()
```

---

## Token Counting Flow

```
[LLM Response Chunk] "Government regulation is necessary"
   â”‚
   â–¼
[Backend] Token Counter
   â”‚
   â”œâ”€ Use tiktoken library:
   â”‚  â””â”€ tokens = tiktoken.encode("Government regulation is necessary")
   â”‚     â””â”€ Result: [38, 5435, 374, 5995] (4 tokens)
   â”‚
   â”œâ”€ Accumulate for current response:
   â”‚  â””â”€ participant_tokens["part_1"] += 4
   â”‚
   â”œâ”€ Calculate cost:
   â”‚  â”œâ”€ Get model pricing from cache:
   â”‚  â”‚  â””â”€ claude-3-5-sonnet: $3/1M input, $15/1M output
   â”‚  â”œâ”€ Determine token type (assume output for responses):
   â”‚  â”‚  â””â”€ price_per_token = $15 / 1,000,000 = $0.000015
   â”‚  â””â”€ cost_increment = 4 * $0.000015 = $0.00006
   â”‚
   â”œâ”€ Update cost tracker:
   â”‚  â”œâ”€ total_cost += $0.00006
   â”‚  â””â”€ cost_by_model["anthropic/claude"] += $0.00006
   â”‚
   â””â”€ Check if should emit cost_update:
      â””â”€ If 5 seconds elapsed since last update:
         â””â”€ Emit SSE cost_update event
```

---

## Error Handling Flows

### Flow 5A: LLM API Timeout

```
[Backend] Call LLM API with timeout
   â”‚ async with timeout(90):  # 90 second timeout
   â”‚     response = await anthropic_client.stream(...)
   â”‚
   â–¼
[LLM API] Takes too long (> 90 seconds)
   â”‚
   â–¼
[Backend] asyncio.TimeoutError caught
   â”‚
   â”œâ”€ Log error:
   â”‚  â””â”€ logger.error(f"Timeout for participant {part_id}")
   â”‚
   â”œâ”€ Create error:
   â”‚  â””â”€ DebateError(type="timeout", participantId=part_id, retryable=true)
   â”‚
   â–¼
[Backend] Emit SSE error event
   â”‚
   â”‚ event: error
   â”‚ data: {
   â”‚   type: "timeout",
   â”‚   message: "Anthropic API timeout (90s)",
   â”‚   participantId: "part_1",
   â”‚   retryable: true
   â”‚ }
   â”‚
   â–¼
[XState] send({ type: 'ERROR', error })
   â”‚
   â–¼ State: debating â†’ error
   â”‚
   â–¼
[Frontend] Display ErrorDisplay
   â”‚
   â”œâ”€ Message: "Anthropic API timed out"
   â”œâ”€ Participant: "Pro-Regulation (Claude)"
   â”œâ”€ Actions:
   â”‚  â”œâ”€ [Retry] â†’ send({ type: 'RETRY' })
   â”‚  â””â”€ [Stop Debate] â†’ send({ type: 'STOP' })
   â”‚
   â–¼
[User] Clicks "Retry"
   â”‚
   â–¼
[XState] Check canRetry guard
   â”‚ â”œâ”€ error.retryable === true âœ“
   â”‚ â””â”€ retryCount < 3 âœ“
   â”‚
   â–¼ State: error â†’ initializing â†’ debating (resumes)
   â”‚
   â–¼
[Backend] Retry failed participant API call
   â”‚ (with exponential backoff)
```

### Flow 5B: Model API Error (Rate Limit)

```
[Backend] Call OpenAI API
   â”‚
   â–¼
[OpenAI API] Returns 429 Too Many Requests
   â”‚
   â”‚ Response:
   â”‚ {
   â”‚   error: {
   â”‚     message: "Rate limit exceeded",
   â”‚     type: "rate_limit_error",
   â”‚     code: "rate_limit_exceeded"
   â”‚   }
   â”‚ }
   â”‚
   â–¼
[Backend] Catch APIError
   â”‚
   â”œâ”€ Parse retry-after header: 60 seconds
   â”œâ”€ Create error:
   â”‚  â””â”€ DebateError(type="model_error", retryable=true, retryAfter=60)
   â”‚
   â–¼
[Backend] Emit SSE error + automatic retry
   â”‚
   â”‚ event: error
   â”‚ data: {
   â”‚   type: "model_error",
   â”‚   message: "OpenAI rate limit. Retrying in 60s...",
   â”‚   participantId: "part_2",
   â”‚   retryable: true
   â”‚ }
   â”‚
   â–¼
[Frontend] Display temporary error banner
   â”‚ "OpenAI rate limited. Auto-retrying in 60s..."
   â”‚
   â–¼
[Backend] Wait 60 seconds, then retry
   â”‚ await asyncio.sleep(60)
   â”‚ response = await openai_client.stream(...)
   â”‚
   â–¼
[Debate Continues...]
```

---

## State Persistence Flow

```
[XState] State transition occurs
   â”‚ (e.g., debating â†’ judgeEvaluating)
   â”‚
   â–¼
[Frontend] Serialize state snapshot
   â”‚
   â”‚ snapshot = {
   â”‚   debateId: "deb_abc123",
   â”‚   state: "judgeEvaluating",
   â”‚   context: {
   â”‚     currentRound: 5,
   â”‚     rounds: [...],
   â”‚     costTracker: {...},
   â”‚     ...
   â”‚   },
   â”‚   timestamp: Date.now()
   â”‚ }
   â”‚
   â–¼
[Frontend] Store in localStorage
   â”‚ localStorage.setItem('debate_state', JSON.stringify(snapshot))
   â”‚
   â–¼
[Backend] Also persist to PostgreSQL
   â”‚
   â”‚ UPDATE debates
   â”‚ SET
   â”‚   status = 'judge_evaluating',
   â”‚   state_snapshot = $snapshot,
   â”‚   updated_at = NOW()
   â”‚ WHERE id = 'deb_abc123'

   ... Browser refresh or crash ...

[Frontend] Page reload
   â”‚
   â–¼
[Frontend] Check localStorage
   â”‚
   â”‚ const saved = localStorage.getItem('debate_state')
   â”‚
   â–¼
[Frontend] If snapshot exists and recent (< 1 hour):
   â”‚
   â”œâ”€ Restore XState machine state:
   â”‚  â””â”€ machine.start({ snapshot: saved })
   â”‚
   â”œâ”€ Reconnect SSE:
   â”‚  â””â”€ GET /api/v1/debates/deb_abc123/stream
   â”‚     (Backend resumes from saved state)
   â”‚
   â””â”€ Resume debate from last known state
```

---

## Optimistic Updates & Rollback

```
[User] Clicks "Pause Debate"
   â”‚
   â–¼
[Frontend] Optimistic state update (immediate)
   â”‚
   â”œâ”€ XState: State â†’ paused (optimistic)
   â”œâ”€ UI: Disable streaming, show "Paused" badge
   â”‚
   â–¼
[Frontend] POST /api/v1/debates/:id/pause
   â”‚
   â–¼
[Backend] Process pause request
   â”‚
   â”œâ”€ Cancel active LLM streams
   â”œâ”€ Save state snapshot
   â”‚
   â–¼
[Backend] Returns 200 OK
   â”‚
   â”‚ Response:
   â”‚ {
   â”‚   debateId: "deb_abc123",
   â”‚   status: "paused",
   â”‚   pausedAt: "2024-12-03T05:45:00Z"
   â”‚ }
   â”‚
   â–¼
[Frontend] Confirm optimistic update
   â”‚ (State already paused, no action needed)

   ... OR if error occurs ...

[Backend] Returns 500 Internal Server Error
   â”‚
   â–¼
[Frontend] Rollback optimistic update
   â”‚
   â”œâ”€ XState: Revert to previous state (debating)
   â”œâ”€ UI: Show error toast: "Failed to pause"
   â”œâ”€ Re-enable streaming
   â”‚
   â–¼
[User] Sees error message, debate continues
```

---

## Multi-Client Synchronization (Redis Pub/Sub)

```
[Backend Instance 1] Receives SSE connection from Client A
   â”‚
   â–¼
[Backend Instance 2] Receives SSE connection from Client B
   â”‚ (Load balanced, different server)
   â”‚
   â–¼
[Backend Instance 1] LLM stream chunk received
   â”‚
   â”‚ Chunk: "Government regulation"
   â”‚
   â”œâ”€ Forward to Client A via SSE
   â”‚
   â””â”€ Publish to Redis:
      â”‚ PUBLISH debate:deb_abc123 {
      â”‚   event: "participant",
      â”‚   data: { participantId: "part_1", chunk: "Government regulation" }
      â”‚ }
      â”‚
      â–¼
[Redis] Broadcast to all subscribers
   â”‚
   â–¼
[Backend Instance 2] Subscribed to debate:deb_abc123
   â”‚
   â”œâ”€ Receive published message
   â”‚
   â””â”€ Forward to Client B via SSE
      â”‚
      â–¼
[Client A] Sees chunk
[Client B] Sees same chunk (synchronized)
```

---

## Summary of Key Data Flows

| Flow | Trigger | Path | Result |
|------|---------|------|--------|
| **Initialization** | User clicks "Start" | Frontend â†’ Backend â†’ DB â†’ SSE Setup | Debate begins |
| **Round Execution** | Backend orchestrator | Backend â†’ LLM APIs (parallel) â†’ SSE â†’ Frontend | Participants respond |
| **Cost Tracking** | Every LLM response | Token Counter â†’ Cost Calculator â†’ SSE â†’ CostTracker | Real-time cost display |
| **Warning** | Cost threshold | Backend â†’ SSE cost_warning â†’ Modal | User notified |
| **Judge Evaluation** | All rounds complete | Backend â†’ Judge LLM â†’ SSE â†’ JudgePanel | Verdict displayed |
| **Error Handling** | LLM API failure | Backend â†’ SSE error â†’ XState â†’ ErrorDisplay | Error shown + retry option |
| **State Persistence** | Every transition | XState â†’ localStorage + PostgreSQL | Crash-resistant |
| **Multi-Client Sync** | SSE events | Backend â†’ Redis Pub/Sub â†’ All Clients | Multiple viewers synchronized |

---

## Performance Metrics

- **SSE Connection**: < 100ms to establish
- **LLM First Chunk**: 500-1500ms latency (varies by model)
- **Token Counting**: < 1ms per chunk
- **Cost Calculation**: < 1ms (cached pricing)
- **Frontend Render**: < 16ms (60fps) for streaming updates
- **State Persistence**: < 5ms to localStorage
- **Database Write**: < 50ms for state snapshot

---

## Scalability Considerations

1. **Horizontal Scaling**: Redis Pub/Sub enables multi-instance backends
2. **Connection Pooling**: Reuse LLM API connections
3. **Rate Limiting**: Queue requests if approaching API limits
4. **Caching**: Cache debate configs, model pricing, token counts
5. **Database Indexes**: Index debates by status, created_at
6. **CDN**: Static assets (React bundle) served via CDN

---

## Next Steps

1. Implement Backend orchestrator with asyncio for parallel LLM calls
2. Build SSE multiplexer with Redis Pub/Sub
3. Create token counting service with tiktoken
4. Implement frontend SSE handler with event routing
5. Add Zustand stream buffering for optimized React updates
6. Build cost tracking service with real-time updates
7. Implement state persistence (localStorage + PostgreSQL)
8. Add error recovery with exponential backoff
